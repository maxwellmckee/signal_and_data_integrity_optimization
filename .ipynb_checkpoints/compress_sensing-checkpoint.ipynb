{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3874e406",
   "metadata": {},
   "source": [
    "#### Overview: Compress Sensing\n",
    "This focuses on the concept of compressed sensing, specifically the task of recovering an $s$-sparse vector $\\textbf{x}$ from the observed output $\\textbf{y}$, which is the result of multiplying $\\textbf{x}$ by a $256 \\times 512$ matrix $\\textbf{A}$. The recovery process involves solving an $l_1$ minimization problem to find the sparsest solution that still results in the observed output. For varying levels of sparsity ($s$) from 1 to 128, the assignment requires running 20 independent numerical experiments to generate matrix $\\textbf{A}$ and sparse vector $\\textbf{x}$, solve the minimization problem, and then calculate the success rate based on the accuracy of the recovered vector compared to the original.\n",
    "\n",
    "#### Sparse Vector Recovery via Compressed Sensing\n",
    "Consider a scenario where we have a matrix $\\textbf{A}$ of dimensions $256 \\times 512$. Within this context, there exists a vector $\\textbf{x} \\in \\mathbb{R}^{512}$ characterized by its sparsity ($s$-sparse), and the output $\\textbf{y}$, which is obtained through the equation $\\textbf{y = Ax}$. The primary objective of this task revolves around implementing $l_1$ minimization to accurately retrieve $\\textbf{x}$, as defined by:\n",
    "\n",
    "$$ \\min_{\\textbf{x}} \\parallel \\textbf{x} \\parallel_1 \\text{ such that } \\textbf{Ax = y} $$\n",
    "  \n",
    "subject to Ax = y\n",
    "\n",
    "Here, the notation $\\parallel \\textbf{x} \\parallel_1$ denotes the $l_1$ norm of $\\textbf{x}$. With $s$ taking on values from 1 to 128, you are to conduct 20 distinct numerical trials for each $s$ value. In every trial, you'll generate the matrix $\\textbf{A}$ with elements as independent and identically distributed (i.i.d) standard Gaussian random variables. Concurrently, construct $\\textbf{x}$ as a randomly selected $s$-sparse signal, where the nonzero elements are also sourced from a standard Gaussian distribution. The criteria for a trial's success hinges on the solution $\\textbf{x}$ from the minimization satisfying $ \\parallel \\hat{x} - x \\parallel_2 \\leq 10^{-4} \\parallel x \\parallel_2$. Your task is to compute and present the empirical success rates, averaged over the 20 trials, for each sparsity level $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8100f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s=1: success rate=1.0\n",
      "s=2: success rate=1.0\n",
      "s=3: success rate=1.0\n",
      "s=4: success rate=1.0\n",
      "s=5: success rate=1.0\n",
      "s=6: success rate=1.0\n",
      "s=7: success rate=1.0\n",
      "s=8: success rate=1.0\n",
      "s=9: success rate=1.0\n",
      "s=10: success rate=1.0\n",
      "s=11: success rate=1.0\n",
      "s=12: success rate=1.0\n",
      "s=13: success rate=1.0\n",
      "s=14: success rate=1.0\n",
      "s=15: success rate=1.0\n",
      "s=16: success rate=1.0\n",
      "s=17: success rate=1.0\n",
      "s=18: success rate=1.0\n",
      "s=19: success rate=1.0\n",
      "s=20: success rate=1.0\n",
      "s=21: success rate=1.0\n",
      "s=22: success rate=1.0\n",
      "s=23: success rate=1.0\n",
      "s=24: success rate=1.0\n",
      "s=25: success rate=1.0\n",
      "s=26: success rate=1.0\n",
      "s=27: success rate=1.0\n",
      "s=28: success rate=1.0\n",
      "s=29: success rate=1.0\n",
      "s=30: success rate=1.0\n",
      "s=31: success rate=1.0\n",
      "s=32: success rate=1.0\n",
      "s=33: success rate=1.0\n",
      "s=34: success rate=1.0\n",
      "s=35: success rate=1.0\n",
      "s=36: success rate=1.0\n",
      "s=37: success rate=1.0\n",
      "s=38: success rate=1.0\n",
      "s=39: success rate=1.0\n",
      "s=40: success rate=1.0\n",
      "s=41: success rate=1.0\n",
      "s=42: success rate=1.0\n",
      "s=43: success rate=1.0\n",
      "s=44: success rate=1.0\n",
      "s=45: success rate=1.0\n",
      "s=46: success rate=1.0\n",
      "s=47: success rate=1.0\n",
      "s=48: success rate=1.0\n",
      "s=49: success rate=1.0\n",
      "s=50: success rate=1.0\n",
      "s=51: success rate=1.0\n",
      "s=52: success rate=1.0\n",
      "s=53: success rate=1.0\n",
      "s=54: success rate=1.0\n",
      "s=55: success rate=1.0\n",
      "s=56: success rate=1.0\n",
      "s=57: success rate=1.0\n",
      "s=58: success rate=1.0\n",
      "s=59: success rate=1.0\n",
      "s=60: success rate=1.0\n",
      "s=61: success rate=1.0\n",
      "s=62: success rate=1.0\n",
      "s=63: success rate=1.0\n",
      "s=64: success rate=1.0\n",
      "s=65: success rate=1.0\n",
      "s=66: success rate=1.0\n",
      "s=67: success rate=1.0\n",
      "s=68: success rate=1.0\n",
      "s=69: success rate=1.0\n",
      "s=70: success rate=1.0\n",
      "s=71: success rate=1.0\n",
      "s=72: success rate=1.0\n",
      "s=73: success rate=1.0\n",
      "s=74: success rate=1.0\n",
      "s=75: success rate=1.0\n",
      "s=76: success rate=1.0\n",
      "s=77: success rate=1.0\n",
      "s=78: success rate=1.0\n",
      "s=79: success rate=1.0\n",
      "s=80: success rate=1.0\n",
      "s=81: success rate=1.0\n",
      "s=82: success rate=1.0\n",
      "s=83: success rate=1.0\n",
      "s=84: success rate=0.95\n",
      "s=85: success rate=1.0\n",
      "s=86: success rate=0.95\n",
      "s=87: success rate=1.0\n",
      "s=88: success rate=0.95\n",
      "s=89: success rate=0.95\n",
      "s=90: success rate=0.9\n",
      "s=91: success rate=0.75\n",
      "s=92: success rate=0.95\n",
      "s=93: success rate=0.7\n",
      "s=94: success rate=0.8\n",
      "s=95: success rate=0.8\n",
      "s=96: success rate=0.65\n",
      "s=97: success rate=0.3\n",
      "s=98: success rate=0.55\n",
      "s=99: success rate=0.45\n",
      "s=100: success rate=0.45\n",
      "s=101: success rate=0.4\n",
      "s=102: success rate=0.4\n",
      "s=103: success rate=0.3\n",
      "s=104: success rate=0.1\n",
      "s=105: success rate=0.2\n",
      "s=106: success rate=0.2\n",
      "s=107: success rate=0.15\n",
      "s=108: success rate=0.05\n",
      "s=109: success rate=0.15\n",
      "s=110: success rate=0.05\n",
      "s=111: success rate=0.1\n",
      "s=112: success rate=0.05\n",
      "s=113: success rate=0.0\n",
      "s=114: success rate=0.0\n",
      "s=115: success rate=0.05\n",
      "s=116: success rate=0.0\n",
      "s=117: success rate=0.0\n",
      "s=118: success rate=0.0\n",
      "s=119: success rate=0.0\n",
      "s=120: success rate=0.0\n",
      "s=121: success rate=0.0\n",
      "s=122: success rate=0.0\n",
      "s=123: success rate=0.0\n",
      "s=124: success rate=0.0\n",
      "s=125: success rate=0.0\n",
      "s=126: success rate=0.0\n",
      "s=127: success rate=0.0\n",
      "s=128: success rate=0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "\n",
    "# Generate a s-sparse signal\n",
    "def generate_signal(n, s):\n",
    "    x = np.zeros(n)\n",
    "    support = np.random.choice(np.arange(n), size=s, replace=False)\n",
    "    x[support] = np.random.normal(size=s)\n",
    "    return x\n",
    "\n",
    "# Check if experiment is successful\n",
    "def is_successful(x, x_hat, epsilon=1e-4):\n",
    "    return np.linalg.norm(x - x_hat) <= epsilon * np.linalg.norm(x)\n",
    "\n",
    "# Main function for each experiment\n",
    "def run_experiment(m, n, s):\n",
    "    # Step 1: Generate a s-sparse signal\n",
    "    x = generate_signal(n, s)\n",
    "\n",
    "    # Step 2: Generate the matrix A\n",
    "    A = np.random.normal(size=(m, n))\n",
    "\n",
    "    # Step 3: Calculate the observed system output\n",
    "    y = A @ x\n",
    "\n",
    "    # Step 4: Solve the L1 minimization problem\n",
    "    x_hat = cp.Variable(n)\n",
    "    objective = cp.Minimize(cp.norm(x_hat, 1))\n",
    "    constraints = [A @ x_hat == y]\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve()\n",
    "\n",
    "    # Step 5: Check if the experiment is successful\n",
    "    return is_successful(x, x_hat.value)\n",
    "\n",
    "# Main loop over all choices of s\n",
    "def run_experiments(m, n, s_values, num_experiments):\n",
    "    success_rates = []\n",
    "    for s in s_values:\n",
    "        successful_experiments = sum(run_experiment(m, n, s) for _ in range(num_experiments))\n",
    "        success_rate = successful_experiments / num_experiments\n",
    "        success_rates.append(success_rate)\n",
    "    return success_rates\n",
    "\n",
    "# Parameters\n",
    "m = 256\n",
    "n = 512\n",
    "s_values = np.arange(1, 129)\n",
    "num_experiments = 20\n",
    "\n",
    "# m = 32\n",
    "# n = 64\n",
    "# s_values = np.arange(1, 10)\n",
    "# num_experiments = 5\n",
    "\n",
    "# Run experiments and print success rates\n",
    "success_rates = run_experiments(m, n, s_values, num_experiments)\n",
    "for s, success_rate in zip(s_values, success_rates):\n",
    "    print(f's={s}: success rate={success_rate}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f64b7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
