{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b1e045",
   "metadata": {},
   "source": [
    "#### Overview: Matrix Recovery\n",
    "This Python assignment delves into the field of matrix recovery, focusing on reconstructing a low-rank matrix after a portion of its entries has been randomly set to zero. The process involves:\n",
    "\n",
    "Generating a $50 \\times 50$ matrix, $M_0$, of rank $2$, and then randomly zeroing out $50%$ of its entries.\n",
    "Formulating and solving an optimization problem aimed at minimizing the nuclear norm of a matrix $M$, under the constraint that $M$ matches $M_0$ in all the entries that were not set to zero.\n",
    "Calculating and reporting the relative reconstruction error between the original matrix $M_0$ and the recovered matrix $M$, using the Frobenius norm.\n",
    "\n",
    "#### Low-Rank Matrix Reconstruction\n",
    "\n",
    "(a) We create a random $50 \\times 50$ matrix of rank $2$, denoted as $M_0$. Then, we simulate a data loss scenario by randomly replacing $50%$ of $M_0$'s entries with zeros.\n",
    "\n",
    "(b) We approach the challenge of reconstructing the original matrix by tackling this optimization task: \n",
    "\n",
    "$$ \\min_M \\parallel M \\parallel_* $$\n",
    "\n",
    "ensuring that for every $(i,j)$ pair belonging to the subset of entries that remained unchanged, $M(i,j)$ equals $M_0(i,j)$.\n",
    "\n",
    "(c) We evaluate the success of your reconstruction effort by computing and reporting the relative reconstruction error. This metric is calculated as follows: \n",
    "\n",
    "$$ \\frac{\\parallel M - M_0 \\parallel_F}{\\parallel M_0 \\parallel_F} $$\n",
    "\n",
    ",where the comparison involves the reconstructed matrix $M$ and the original matrix $M_0$, using the Frobenius norm as the measure of discrepancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7530f7c2",
   "metadata": {},
   "source": [
    "### Simulate data loss\n",
    "To create a rank-2 matrix we can take the outer product of two random vectors. Then we will make 50% of the entries 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab75f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "\n",
    "# Two random vectors\n",
    "v1 = np.random.rand(50, 1)\n",
    "v2 = np.random.rand(1, 50)\n",
    "\n",
    "# Create a rank-2 matrix by taking the outer product\n",
    "M0 = v1 @ v2\n",
    "\n",
    "# Flatten the matrix and create a boolean mask where 50% of the entries are True\n",
    "mask = np.random.choice([True, False], size=M0.size, p=[0.5, 0.5])\n",
    "\n",
    "# Reshape the mask to the same shape as M0\n",
    "mask = mask.reshape(M0.shape)\n",
    "\n",
    "# Replace the entries where the mask is True with 0\n",
    "M0[mask] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47321c70",
   "metadata": {},
   "source": [
    "### Original matrix reconstruction\n",
    "\n",
    "We will solve the nuclear norm minimization problem which can be used for low-rank matrix completion. First, `cp.Variable` will create a variable `M` to represent the completed matrix. `cp.Minimize` and `cp.norm` define the objective function, and the `known set` is where the original `M0` was not masked. We then create the constraints. `cp.Problem` is an object created witht he objective and constraints, and then `problem.solve()` solves it. Lastly, the `solver=cp.SCS` is a solver for convex optimization problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46031bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable that has the same shape as M0\n",
    "M = cp.Variable(shape=M0.shape, name=\"M\")\n",
    "\n",
    "# Objective function - nuclear norm of M\n",
    "objective = cp.Minimize(cp.norm(M, \"nuc\"))\n",
    "\n",
    "# Create the constraints that M[i,j] = M0[i,j] for all (i,j) in the known set\n",
    "known_set = np.where(mask == False)  # assuming mask is the boolean mask used to generate M0\n",
    "constraints = [M[i,j] == M0[i,j] for i,j in zip(*known_set)]\n",
    "\n",
    "# Create problem\n",
    "problem = cp.Problem(objective, constraints)\n",
    "\n",
    "# Solve problem\n",
    "problem.solve(solver=cp.SCS)\n",
    "\n",
    "# Optimal value for M\n",
    "M_opt = M.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e60e7e",
   "metadata": {},
   "source": [
    "### Evaluation of reconstruction success\n",
    "The relative reconstruction error can be calculated using the Frobenius norm. `np.linalg.norm(M_opt - M0, 'fro')` computes the Frobenius norm of the difference between `M_opt` and `M0`. A similar thing is done for `np.linalg.norm(M0, 'fro')`. The error is low if `M_opt` is a good reconstruction of `M0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec4c50cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Reconstruction Error:  0.9314082338269389\n"
     ]
    }
   ],
   "source": [
    "relative_error = np.linalg.norm(M_opt - M0, 'fro') / np.linalg.norm(M0, 'fro')\n",
    "print(\"Relative Reconstruction Error: \", relative_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3b894b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "general_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
